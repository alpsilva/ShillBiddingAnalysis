{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/alps2/.local/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alps2/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/alps2/.local/lib/python3.10/site-packages (from scikit-learn) (1.23.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/alps2/.local/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/alps2/.local/lib/python3.10/site-packages (from scikit-learn) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/alps2/.local/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/alps2/.local/lib/python3.10/site-packages (from seaborn) (3.5.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/alps2/.local/lib/python3.10/site-packages (from seaborn) (1.23.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/alps2/.local/lib/python3.10/site-packages (from seaborn) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/alps2/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.36.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/alps2/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alps2/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/alps2/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/alps2/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/alps2/.local/lib/python3.10/site-packages (1.3.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/alps2/.local/lib/python3.10/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/alps2/.local/lib/python3.10/site-packages (from pandas) (1.23.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install pandas\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337\n",
    "N_JOBS = -1\n",
    "FOLDS = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas\n",
    "\n",
    "def load_data(file_path: str, positive_label_multiplication: int = 2) -> pandas.DataFrame:\n",
    "    \"\"\" Receives a file path for the dataset training, testing and validation datasets. \"\"\"\n",
    "\n",
    "    # Loading data from csv file\n",
    "    df = pandas.read_csv(file_path)\n",
    "\n",
    "    # Selecting useful features\n",
    "    useful_features = [\n",
    "        \"Bidder_Tendency\",\n",
    "        \"Bidding_Ratio\",\n",
    "        \"Successive_Outbidding\",\n",
    "        \"Last_Bidding\",\n",
    "        \"Auction_Bids\",\n",
    "        \"Starting_Price_Average\",\n",
    "        \"Early_Bidding\",\n",
    "        \"Winning_Ratio\",\n",
    "        \"Auction_Duration\",\n",
    "        \"Class\"\n",
    "    ]\n",
    "\n",
    "    df = df[useful_features]\n",
    "\n",
    "    # Augmenting positive label data\n",
    "    positive_labels = df[df[\"Class\"] == 1]\n",
    "\n",
    "    dfs_to_concat = [df]\n",
    "    for _ in range(positive_label_multiplication):\n",
    "        dfs_to_concat.append(positive_labels)\n",
    "\n",
    "    df = pandas.concat(dfs_to_concat)\n",
    "    df = df.sample(frac=1)\n",
    "\n",
    "    # Separating features and labels\n",
    "    columns = list(df.columns)\n",
    "    features = columns[:len(columns)-1]\n",
    "    label = columns[len(columns)-1:]\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[label]\n",
    "\n",
    "    # Creating training, testing and validation datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = RANDOM_SEED)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.125, random_state = RANDOM_SEED)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./dataset/Shill Bidding Dataset.csv\"\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_valid, y_valid = load_data(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarão de função para treinamento de modelos\n",
    "\n",
    "A ferramenta gridsearch será usada para determinar a melhor combinação de parâmetros para cada modelo.\n",
    "\n",
    "Uma função search é definida, que recebe uma instância de modelo a ser \"variado\" e um dicionário de parâmetros e suas variações. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(model, model_parameters): \n",
    "    \"\"\" \"\"\"\n",
    "    categorical_features = [\n",
    "        \"A1_Score\",\n",
    "        \"A2_Score\",\n",
    "        \"A3_Score\",\n",
    "        \"A4_Score\",\n",
    "        \"A5_Score\",\n",
    "        \"A6_Score\",\n",
    "        \"A7_Score\",\n",
    "        \"A8_Score\",\n",
    "        \"A9_Score\",\n",
    "        \"A10_Score\",\n",
    "        \"gender\",\n",
    "        \"ethnicity\",\n",
    "        \"jundice\",\n",
    "        \"country\",\n",
    "        \"used_app_before\",\n",
    "        \"relation\"\n",
    "    ]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_pipeline = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model_pipeline,\n",
    "        model_parameters,\n",
    "        n_jobs=N_JOBS,\n",
    "        cv=FOLDS\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best score after optimization: {grid_search.best_score_}\")\n",
    "    print(\"Best params:\")\n",
    "    for key, value in grid_search.best_params_.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_parameters = {\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"clf__splitter\": [\"best\", \"random\"],\n",
    "    \"clf__max_depth\": range(3, 50),\n",
    "    \"clf__min_samples_split\": range(2, 10)\n",
    "}\n",
    "\n",
    "gs_knn = search(KNeighborsClassifier(), knn_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando LVQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando Decision Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando Rede Neural MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando Comitê de redes neurais Artificiais"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variando Comitê Heterogêneo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
